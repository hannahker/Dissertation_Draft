\chapter{Literature Review}
\label{chapterlabel2}

In this literature review, I will introduce the OpenStreetMap (OSM) project, situating it within the broader phenomena of volunteered geographic information, neogeography, and the Web 2.0. I will then address key issues relating to data quality in OSM and review the large volume of past work that has addressed this topic. I highlight the trend of work that has moved from extrinsic to intrinsic quality assessments and identify a need to address temporal accuracy in greater depth. I next focus more closely on the issue of temporal accuracy in OSM and discuss the dynamics of editing. I then focus on the case of humanitarian mapping and discuss the applications of OSM in humanitarian contexts and the unique modes of data production in this domain. I conclude by situating the work of this thesis in the research gap that exists at the intersection of data maintenance, as a dimension of temporal data quality, and humanitarian mapping efforts. 

\section{Introduction to OSM}

OSM is an example of what \textcite{goodchild_citizens_2007} terms, "volunteered geographic information" (VGI). VGI sits under the umbrella of "neogeography", in which the democratization of tools for geospatial data production and consumption lead to a wealth of citizen-generated geospatial datasets \parencite{goodchild_neogeography_2009, haklay_web_2008}. More broadly, neogeography is enabled by the rise of the Web 2.0, in which the lines between content production and content consumption on the web are blurred \parencite{oreilly_what_2009}. 

Popularly framed as the "Wikipedia of maps", OSM seeks to empower individuals to share their local spatial knowledge to create a crowdsourced map of the world \parencite{fox_openstreetmap_2012, lu_after_2019}. Theoretically, anyone with access to the internet can contribute to OSM. At the time of writing, OSM has over 6 million registered users (although it is likely that not all users have contributed data) and almost 8 billion uploaded GPS points \parencite{noauthor_openstreetmap_2020}. OSM offers a free alternative to proprietary geospatial datasets, and is used for purposes such as vehicle routing \parencite{graser_is_2015, luxen_real-time_2011} and POI searching \parencite{ruta_indooroutdoor_2015}. The OSM contribution landscape is also very heterogeneous. Large-scale, existing geospatial datasets can be imported into OSM, such as the US TIGER import in 2008 \parencite{zielstra_assessing_2013}. Moreover, corporate entities; such as Facebook, Microsoft, and Apple; are increasingly involved in mapping efforts \parencite{anderson_corporate_2019}. Recent efforts, such as Facebook's \textit{mapwith.ai} tool\footnote{\url{https://mapwith.ai/}}, have also sought to employ machine learning techniques to automatically (or with minimal human supervision) create OSM features from satellite imagery \parencite{albrecht_change_2020, yadav_human_2020}. 

\section{Data Quality and OSM}

As a crowdsourced dataset, one of the primary potential issues with OSM is its quality. OSM does not provide any assurances of its quality; unlike traditional, authoritative geospatial datasets. Moreover, its contributors do not require any formal training or qualifications. Questions of data quality are also particularly relevant and challenging to address in this context because of the highly diverse nature of contributions and contributors, leading to variable quality throughout  \parencite{girres_quality_2010, grochenig_digging_2014, haklay_how_2010, neis_analyzing_2012}. 

Existing literature identifies numerous dimensions of geospatial data quality, such as completeness, logical consistency, positional accuracy, temporal accuracy, and usability \parencite{antoniou_measures_2015, fox_notion_1994, van_oort_spatial_2006}. Questions of VGI data quality are also framed around the concepts of trust and credibility, reminding one of the presence of the data user who must evaluate the fitness of the data for their task at hand \parencite{flanagin_credibility_2008, severinsen_vgtrust_2019}. While a characteristic such as positional accuracy can be empirically evaluated, trust and credibility are perceptual qualities of a dataset that relate to its "believability" in the eyes of the data consumer \parencite{flanagin_credibility_2008}. \textcite{barron_comprehensive_2014} also address the notion of fitness for use in their consideration of geospatial data quality, demonstrating how different applications of a given dataset will require different quality needs. 

The framework laid out by \textcite{goodchild_assuring_2012} is particularly useful in understanding the mechanisms for quality control in VGI projects. The authors outline the following three approaches: 1) the \textit{crowdsourcing approach}, as evaluated by \textcite{haklay_how_2010-1}, by which a community of contributors will converge on the "truth" and correct the errors of others; 2) the \textit{social approach}, by which contributors are organized in a hierarchy with those at the top acting as content moderators or gatekeepers; and the 3) \textit{geographic approach}, by which common-sense rules about the nature of geographic phenomena are used to filter out clear errors \parencite{goodchild_assuring_2012}. This framework demonstrates how the community structure of VGI projects such as OSM can contribute to enhanced quality control, however additional technical checks and evaluation may be needed.

Efforts to empirically assess the quality of OSM data began with so-called "extrinsic" approaches, whereby OSM data is compared against an authoritative dataset of assumed high quality. \textcite{haklay_how_2010} compares the completeness and positional accuracy of OSM data in England with that from the Ordnance Survey. \textcite{girres_quality_2010} extend this analysis to the French OSM dataset and \textcite{zielstra_comparative_2010} compare OSM data in Germany to that from the TeleAtlas MultiNet dataset. Overall, these works find OSM data to be of relatively high quality, however quality can also be quite variable across both geographic space and across the different elements of geospatial data quality \parencite{girres_quality_2010, haklay_how_2010}. For example, \textcite{zielstra_comparative_2010} find significant differences in completeness between urban and rural areas in Germany, with rural areas needing greater coverage.

More recent efforts to assess OSM data quality are trending towards intrinsic approaches. Such efforts can be defined as "process-based measures focusing on pragmatic or contextual ‘authority’ by examining the processes generating information" \parencite[p. 297]{anderson_crowd_2018}. Intrinsic efforts may be preferable due to the potential high cost of obtaining proprietary datasets, or the lack of availability of such reference datasets \parencite{estes_maps_1994}. Intrinsic quality assessment may also be more appropriate as one acknowledges that authoritative, reference datasets may not be of sufficiently high quality themselves, as suggested by \textcite[p. 112]{goodchild_assuring_2012}. One of the first efforts at intrinsic quality assessment was conducted by \textcite{haklay_how_2010-1}, who empirically evaluate the positive (although non-linear) relationship between positional accuracy and the number of contributors for a given local region. The "Crowd Quality" framework suggested by \textcite{van_exel_impact_2010} demonstrates how many intrinsic approaches will use characteristics of OSM contributors and history of the data as an indication of quality. \textcite{barron_comprehensive_2014} offer one of the most comprehensive frameworks for intrinsic quality assessment through their \textit{iOSMAnalyzer} tool, which combines over 25 data quality indicators that are tailored to different application areas. 

\section{Temporal data quality and the evolution of OSM data}

Temporal accuracy, or up-to-date-ness, is an important element of geospatial data quality \parencite{van_oort_spatial_2006}. As stated by \textcite[p. 884]{barron_comprehensive_2014}, "Ideally the process of updating the OSM features’ geometries and attributes is carried out continuously, homogeneously, throughout and is not limited to speciﬁc features." This statement leads one to consider temporal accuracy as the "validity" or "currency" of data at a given point in time \parencite[p. 17]{van_oort_spatial_2006}. Put simply, when the real world changes but its associated geospatial data does not, it can be considered as out of date (or temporally inaccurate).	

Dimensions of temporal accuracy, however, have received comparatively little direct focus in existing work on OSM. Early quality assessments of OSM largely focused on completeness and positional accuracy of the data, as asserted by \textcite[p.83]{neis_recent_2014}, and exemplified by the work of \textcite{haklay_how_2010-1, haklay_how_2010, helbich_comparative_2012}. The temporal dimension of OSM (ie. the evolution of its data over time) has been given attention, but this focus has mostly been to look at aspects of data quality such as completeness \parencite{grochenig_estimating_2014}. While completeness and positional accuracy of OSM are undeniably critical for many applications, one must not disregard the many other elements of geospatial data quality, which may need to be prioritized in some applications. 

Nevertheless, some selected works have made an effort to address temporal accuracy in OSM data. These efforts are largely intrinsic and investigate factors such as the relationship between volume of active contributors in a region and the frequency of feature updates \parencite{girres_quality_2010}. Past work has also used a feature's date of last edit as an indication of its recency \parencite{minghini_open_2018, roick_technical_2012}. However, intrinsically assessing a feature’s currency by looking at its date of last edit may be an inaccurate assessment, as there is no indication of whether or not the corresponding real-world feature has changed since that date \parencite{barron_comprehensive_2014}. In areas where reference data is not available, this shortcoming suggests a need develop more in-depth understandings of the dynamics of contributions and the lineage of the dataset. 

When considering temporal accuracy, it is valuable to take a step back to think more deeply about the dynamics of editing OSM. As in the real world, geospatial entities in OSM do not necessarily remain consistent over time. The preservation of historical OSM data has allowed the characteristics and dynamics of change to be the subject for various research efforts. As the speed of mapping varies significantly over space, one can consider OSM as containing a multitude of overlapping maps, each along different trajectories to completeness \parencite{chuang_one_2013}. The lineage of OSM data thus comes to the forefront of research efforts, such as in the case of \textcite{mooney_characteristics_2012}, who investigate the characteristics of features that have histories of significant revision; or with  \textcite{trame_exploring_2011}, who use heat maps to visualize the heterogeneous spatial patterns of feature edits over time. While this work reveals highly dynamic processes of editing and contributing, there has been evidence of stagnation in some parts of OSM \parencite{grochenig_digging_2014}. While reductions in editing activities may indicate saturation in the dataset (and thus suggest high completeness), this may also indicate places with low contributor engagement, as people are unwilling or unable to maintain the data \parencite{grochenig_digging_2014}.

Efforts to understand the prevalence of data maintenance offer a promising approach for helping us to evaluate the temporal accuracy of OSM data for a given area. \textcite{mcconchie_wiki_2013} provides a powerful metaphor, likening data maintenance in OSM to a process of "map gardening". Accordingly, one can understand maintenance as the activity by which data is kept up-to-date. Ongoing data maintenance is thus necessary for ensuring that OSM data is of high quality. The identification of data maintenance activities is grounded in the distinction between different forms of OSM contributions, used to identify different phases in the development of the map for a given region \parencite{anderson_crowd_2018}. It is thought that a data maintenance phase is reached once the map for a given area reaches a sufficient level of maturity, when the bulk of editing activity shifts from the addition of new features to the modification of existing ones \parencite{anderson_crowd_2018}. \textcite{quattrone_work_2017} offer the most in-depth assessment of data maintenance of OSM, looking globally at maintenance practices of point of interest (POI) data. The need for maintenance is also not unique to OSM, and has been identified in other crowd-produced knowledge repositories, such as Wikipedia \parencite{kittur_he_2007}. 

\section{OSM production and use in humanitarian contexts}

It is acknowledged that quality assessments of OSM data should be grounded in specific application area(s) \parencite{barron_comprehensive_2014}. In this work, I focus specifically on the context of humanitarian mapping contexts.

High-quality information is critical to successful humanitarian and disaster response work \parencite{cowan_geospatial_2011, poser_volunteered_2010, soden_infrastructure_2016, zook_volunteered_2010}. Geospatial information is necessary for functions such as promoting situational awareness during a crisis (for example, to alert responders to the locations of flood damage, as in \textcite{poser_volunteered_2010}) and ensuring access to resources (for example, by helping to manage the deliveries of supplies to remote villages in Nepal following the 2015 earthquake, as in \textcite{soden_infrastructure_2016}). OSM data is particularly relevant in this context as many of the locations in need of humanitarian assistance do not have any authoritative geospatial data available \parencite{zook_volunteered_2010}. 

Following the 2010 Haiti earthquake, for example, responders needed more detailed and accurate geospatial information about the affected area  \parencite{meier_crisis_2012, soden_infrastructure_2016, zook_volunteered_2010}. Following rapid, remote volunteer efforts to trace recent satellite imagery, OSM became the most complete and up-to-date map of the impacted area \parencite{soden_crowdsourced_2014}. This OSM data was used by many aid agencies, including UNICEF and OCHA \parencite{soden_crowdsourced_2014}. These efforts led to the formation of the Humanitarian OpenStreetMap Team (HOT), which has since led numerous humanitarian mapping efforts in response to natural disasters such as earthquakes and epidemics \parencite{dittus_mass_2017}. 

Such cases of crisis mapping constitute a distinct sub-community within OSM, with unique modes of community organization and data production. Firstly, mapping efforts are largely driven by discrete tasks \parencite{vyncke_personal_2020}. This is exemplified in the HOT Tasking Manager\footnote{The Tasking Manager is a web-based tool that is used to coordinate the mapping activities of an often large and geographically dispersed volunteer mapper community: \url{https://tasks.hotosm.org/}}, where each mapping task is accompanied by a detailed and specific set of instructions. These tasks are often driven by the needs of organizations such as Médecins Sans Frontières (MSF) and the Red Cross who are directly engaged in humanitarian work in the field. Secondly, the majority of contributors are remote \parencite{eckle_quality_2015, vyncke_personal_2020}. Remote contributors add basic geometries (such as building footprints or roads) by tracing satellite imagery \parencite{vyncke_humanitarian_2015}. While it is a best-practice in the humanitarian community to build and engage communities of local stakeholders in the mapping process (as \textcite{soden_crowdsourced_2014} describe in the Haiti example), this may not always be feasible to the extent desired \parencite{vyncke_personal_2020}. These large remote volunteer contributors may be particularly needed as aid organizations may only have a few hired GIS practitioners \parencite[p. 2798]{soden_infrastructure_2016}. Thirdly, the data is often more collaboratively produced than in traditional OSM contexts \parencite{poiani_potential_2016, vyncke_personal_2020}. While all of OSM is a collaborative effort between the entire community of contributors, geographically dispersed humanitarian mapmakers must often be more explicitly coordinated through tools such as the HOT Tasking Manager and mailing list \parencite{palen_success_2015}. Lastly, areas under humanitarian focus are likely to show a distinct spatial pattern, whereby an area of interest has much more coverage than nearby regions on the map \parencite{anderson_crowd_2018}. 

These distinct characteristics of data production in OSM during humanitarian mapping efforts have implications for the quality of the data. When one considers data quality as a dataset’s "fitness for use", the rapidly changing circumstances in crisis scenarios means that elements of temporal data quality are particularly relevant \parencite{chen_coordination_2008}. The bursts of mapping work done by volunteers in the wake of a crisis, as discussed by \textcite{dittus_mass_2017}, may provide more up-to-date spatial data than previously existed \parencite{soden_crowdsourced_2014}. For this reason, the Government of Nepal’s Survey Department published maps containing OSM data on official platforms following the 2015 earthquake \parencite{soden_infrastructure_2016}. However, one can speculate that this rapid creation of new data by remote volunteers may mean that it is challenging to be maintained over time (especially if it is very detailed), and is thus more likely to be out-of-date when the crisis subsides. Efforts that directly assess the quality of data contributed to OSM in humanitarian mapping efforts have been limited. Moreover, there is need for research that directly addresses issues of data maintenance during these mapping efforts. 

\section{Summary of research opportunity}

OSM is a valuable data source that is particularly relevant in humanitarian and disaster relief operations. However, OSM’s lack of official mechanisms for quality assurance and crowdsourced nature raises many concerns around its quality. As evidenced by the large volume of past work, thinking about geospatial data quality in OSM is not new. However, there is a need for more in-depth considerations of temporal accuracy in OSM data, particularly those that consider the unique modes of data production in humanitarian contexts. The concept of data maintenance offers a means to investigate temporal accuracy, and is thus the primary focus of this work. To our knowledge, this work is the first effort to explicitly quantify data maintenance at a local scale following humanitarian mapping campaigns. 