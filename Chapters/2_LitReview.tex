\chapter{Literature Review}
\label{chapterlabel2}

In this literature review, we will introduce the OpenStreetMap project, situating it within the broader phenomena of volunteered geographic information, neogeography, and Web 2.0. We will then address key issues relating to data quality in OSM and review the large volume of past work that has addressed this topic. We highlight the trend of work that has moved from extrinsic to intrinsic quality assessments and identify a need to address temporal accuracy of data in greater depth. We next focus more closely on the issue of temporal accuracy in OSM and discuss the dynamics of editing. We then focus on the case of humanitarian mapping and discuss the applications of OSM in humanitarian contexts and the unique modes of data production in this domain. We conclude by situating the work of this thesis in the research gap that exists at the intersection of data maintenance, as a dimension of temporal data quality, and humanitarian mapping efforts. 

\section{Introduction to OSM}

OSM is a primary example of what \textcite{goodchild_citizens_2007} terms, "volunteered geographic information" (VGI). VGI sits under the umbrella of "neogeography", in which the democratization of tools for geospatial data production and consumption lead to a wealth of citizen-generated geospatial datasets \parencite{goodchild_neogeography_2009, haklay_web_2008}. More broadly, neogeography is enabled by the rise of the Web 2.0, in which the lines between content production and content consumption on the web are blurred \parencite{oreilly_what_2009}. 

Often framed as the "Wikipedia of maps", OSM values citizens' local knowledge and seeks to empower individuals to share their local spatial knowledge with the wider community. Theoretically, anyone with access to the internet can contribute to OSM. At the time of writing, OSM has over 6 million registered users (although it is likely that not all users have contributed data) and almost 8 billion uploaded GPS points \parencite{noauthor_openstreetmap_2020}. OSM offers a free alternative to proprietary geospatial datasets, and is used for purposes such as vehicle routing \parencite{graser_is_2015, luxen_real-time_2011} and POI searching \parencite{ruta_indooroutdoor_2015}. The OSM contribution landscape is also very heterogeneous. Increasingly, large-scale, existing geospatial datasets can be imported into OSM, such as the US TIGER import in 2008 \parencite{zielstra_assessing_2013}. Moreover, corporate entities; such as Facebook, Microsoft, and Apple; are also increasingly involved in mapping efforts \parencite{anderson_corporate_2019}.

\section{Data Quality and OSM}

As a crowdsourced dataset, one of the primary potential issues with OSM is its quality. OSM does not provide any assurances of its quality, unlike traditional, authoritative geospatial datasets. Moreover, its contributors do not require any formal training or qualifications. Questions of data quality are also particularly relevant and challenging to address in this context because of the highly diverse nature of contributions and contributors, leading to variable quality throughout  \parencite{grochenig_digging_2014, haklay_how_2010, neis_analyzing_2012, girres_quality_2010}. 

Existing literature identifies numerous dimensions that should be considered within the concept of geospatial data quality. Dimensions such as completeness, logical consistency, positional accuracy, temporal accuracy (or currentness), and usability are frequently addressed \parencite{fox_notion_1994, antoniou_measures_2015, van_oort_spatial_2006}. Questions of VGI data quality are also framed around the concepts of trust and credibility, reminding one of the presence of the data user who must evaluate the fitness of the data for their task at hand \parencite{flanagin_credibility_2008, severinsen_vgtrust_2019}. While a characteristic such as positional accuracy can be empirically evaluated, trust and credibility are more perceptual qualities of a dataset that relate to its 'believability' in the eyes of the data consumer \parencite{flanagin_credibility_2008}. \textcite{barron_comprehensive_2014} also address the notion of fitness for use in their consideration of geospatial data quality, demonstrating how different applications of a given dataset will require different quality needs. 

The framework laid out by \textcite{goodchild_assuring_2012} is particularly useful in understanding the mechanisms for quality control in VGI projects. The authors outline the following three approaches: 1) the \textit{crowdsourcing approach}, as evaluated by \textcite{haklay_how_2010-1}, by which a community of contributors will converge on the 'truth' and correct the errors of others; 2) the \textit{social approach}, by which contributors are organized in a hierarchy with those at the top acting as content moderators or gatekeepers; and the 3) \textit{geographic approach}, by which common-sense rules about the nature of geographic phenomena are used to filter out clear errors. From these approaches, we can see that the community structure of VGI projects, such as OSM, can contribute to enhanced quality control, however additional technical checks and evaluation may be needed.

Efforts to empirically assess the quality of OSM data began with what are termed 'extrinsic' approaches, whereby OSM data is compared against an authoritative dataset of assumed high quality. \textcite{haklay_how_2010} compares the completeness and positional accuracy of OSM data in England with that from the Ordnance Survey. \textcite{girres_quality_2010} extend this analysis to the French OSM dataset and \textcite{zielstra_comparative_2010} compare OSM data in Germany to that from the TeleAtlas MultiNet dataset. Overall, these works find OSM data to be of relatively high quality, however quality can also be quite variable across both geographic space and across the different elements of geospatial data quality. For example, \textcite{zielstra_comparative_2010} find significant differences in completeness between urban and rural areas in Germany, with rural areas needing greater coverage. Given that the above work was completed relatively early in the development of OSM (within 6 years of the project beginning), the focus was mostly on completeness of coverage and volume of data.  

More recent efforts to assess OSM data quality are trending towards 'intrinsic' quality assessments. Such efforts can be defined as "process-based measures focusing on pragmatic or contextual ‘authority’ by examining the processes generating information" \parencite[p. 297]{anderson_crowd_2018}. Intrinsic efforts may be preferable due to the potential high cost of obtaining proprietary datasets, or the lack of availability of such reference datasets \parencite{estes_maps_1994}. Intrinsic quality assessment may also be more appropriate as we acknowledge that authoritative, reference datasets may not be of sufficiently high quality themselves, as suggested by \textcite[p. 112]{goodchild_assuring_2012}. One of the first efforts at intrinsic quality assessment was conducted by \textcite{haklay_how_2010-1}, who empirically evaluate the positive (although non-linear) relationship between positional accuracy and the number of contributors for a given local region. \textcite{barron_comprehensive_2014} also create the \textit{iOSMAnalyzer} tool, which combines over data quality 25 indicators that are tailored to different application areas. Many efforts at intrinsic quality assessment turn towards the contributor to help understand the quality of their contributions. \textcite{van_exel_impact_2010}, for example, offer a 'Crowd Quality' framework that employs a two-dimensional approach that investigates both attributes of the contributor and attributes of the data in quality assessments 

\section{Temporal data quality and the evolution of OSM data}

Temporal accuracy, or up-to-date-ness, is an important element of geospatial data quality. As stated by \textcite[p. 884]{barron_comprehensive_2014}, "Ideally the process of updating the OSM features’ geometries and attributes is carried out continuously, homogeneously, throughout and is not limited to speciﬁc features." This statement leads one to consider temporal accuracy as the "validity" or "currency" of data at a given point in time \parencite[p. 17]{van_oort_spatial_2006}. Put simply, when the real world changes but its associated geospatial data does not, we consider this dataset to be out of date.	

While temporal accuracy is commonly cited as a key element of overall geospatial data quality, it has received comparatively little direct focus in existing work on OSM. Early quality assessments of OSM largely focused on completeness and positional accuracy of the data, as asserted by \textcite[p.83]{neis_recent_2014}, and exemplified by the work of \textcite{haklay_how_2010-1, haklay_how_2010, helbich_comparative_2012}. The temporal dimension of OSM (ie. the evolution of its data over time) has been given attention, but this focus has mostly been to look at aspects of data quality such as completeness \parencite{grochenig_estimating_2014}. While completeness and positional accuracy of OSM are undeniably critical for many applications, one must not disregard the many other elements of geospatial data quality. 

Only recently have more in-depth assessments of temporal accuracy in OSM been conducted. These efforts are largely intrinsic and investigate factors such as the relationship between volume of active contributors in a region and the frequency of feature updates \parencite{girres_quality_2010}. Past work has also used a feature's date of last edit as an indication of its recency \parencite{minghini_open_2018, roick_technical_2012}. However, intrinsically assessing a feature’s currency by looking at its date of last edit may be an inaccurate assessment, as there is no indication of whether or not the corresponding real-world feature has changed since that date \parencite{barron_comprehensive_2014}. In areas where reference data is not available, this shortcoming suggests a need develop more in-depth understandings of the dynamics of contributions and the lineage of the dataset. 

When considering temporal accuracy, we must take a step back to think more deeply about the dynamics of editing OSM. As in the real world, geospatial entities in OSM do not necessarily remain consistent over time. Thanks to the preservation of historical data in OSM, the characteristics and dynamics of change have been the subject for a variety of research efforts. As the speed of mapping varies significantly over space, we can consider OSM as containing a multitude of overlapping maps, each along different trajectories to completeness \parencite{chuang_one_2013}. The lineage of OSM data thus comes to the forefront of research efforts, such as in the case of \textcite{mooney_characteristics_2012}, who investigate the characteristics of features that have histories of significant revision; or with  \textcite{trame_exploring_2011}, who use heat maps to visualize the heterogeneous spatial patterns of feature edits. Here, the editing history of a given feature is thought to be suggestive of its quality. While this work reveals highly dynamic processes of editing and contributing, there has been evidence of stagnation in some parts of the map \parencite{grochenig_digging_2014}. While reductions in editing activities may indicate saturation in the dataset (and thus suggest high completeness), this may also indicate places with low contributor engagement, as people are unwilling or unable to maintain the data \parencite{grochenig_digging_2014}.

Efforts to understand the prevalence of data maintenance offer a promising approach for helping us to understand how up to date OSM data may be for a given area. Alan McConchie provides a powerful metaphor, likening data maintenance in OSM to a process of "map gardening" \parencite{mcconchie_wiki_2013}. Accordingly, we can understand maintenance as the \textit{activity} by which data is kept up-to-date. Ongoing data maintenance is thus necessary for ensuring that OSM data is of high quality. The identification of data maintenance activities is grounded in the distinction between different forms of OSM contributions, used to identify different phases in the development of the map for a given region \parencite{anderson_crowd_2018}. It is thought that a data maintenance phase is reached once the map for a given area reaches a sufficient level of maturity, when the bulk of editing activity shifts from the addition of new features to the modification of existing ones \parencite{anderson_crowd_2018}. \textcite{quattrone_work_2017} offer the most in-depth assessment of data maintenance of OSM, looking globally at maintenance practices of POI data. The need for maintenance is also not unique to OSM, and has been identified in other crowd-produced knowledge repositories, such as Wikipedia \parencite{kittur_he_2007}. 

\section{OSM production and use in humanitarian contexts}

High-quality information is a critical element to successful humanitarian and disaster response work \parencite{cowan_geospatial_2011, poser_volunteered_2010, soden_infrastructure_2016, zook_volunteered_2010}. Geospatial information is necessary for functions such as promoting situational awareness during a crisis (for example, to alert responders to the locations of flood damage, as in \textcite{poser_volunteered_2010}) and ensuring access to resources (for example, by helping to manage the deliveries of supplies to remote villages in Nepal following the 2015 earthquake, as in \textcite{soden_infrastructure_2016}). OSM data is particularly relevant in this context as many of the locations in need of humanitarian assistance do not have any authoritative geospatial data available \textcite{zook_volunteered_2010}. 

The Haiti earthquake in 2010 is a case where more detailed and accurate geospatial information about the affected area was needed \parencite{meier_crisis_2012, soden_infrastructure_2016, zook_volunteered_2010}. Following rapid, remote volunteer efforts to trace recent satellite imagery, OSM became the most complete and up-to-date map of the impacted area \parencite{soden_crowdsourced_2014}. This OSM data was used by many aid agencies, including UNICEF and OCHA \parencite{soden_crowdsourced_2014}. These efforts led to the formation of the Humanitarian OpenStreetMap Team (HOT), which has since led numerous humanitarian mapping efforts in response to natural disasters such as earthquakes and epidemics \parencite{dittus_mass_2017}. 

Such cases of crisis mapping constitute a distinct sub-community within OSM that must be considered differently than how OSM is used and produced in more populated, urbanized, and industrialized contexts (such as in the UK and Germany). The following statements were validated during personal communication with Jorieke Vyncke, the Missing Maps project coordinator at Médecins Sans Frontières (MSF) \parencite{vyncke_conversation_2020}. 

Firstly, mapping efforts are largely driven by discrete tasks. This is exemplified in the HOT Tasking Manager\footnote{\url{https://tasks.hotosm.org/}}, where each mapping task is accompanied by a detailed and specific set of instructions. These tasks are often driven by the needs of organizations such as MSF and the Red Cross who are directly engaged in humanitarian work in the field. Secondly, the majority of contributors are remote \parencite{eckle_quality_2015}. Remote contributors can only add basic geometries (such as building footprints or roads) by tracing satellite imagery \parencite{vyncke_humanitarian_2015}. While it is a best-practice in the humanitarian community to build and engage communities of local stakeholders in the mapping process (as \textcite{soden_crowdsourced_2014} describe in the Haiti example), this is not always accomplished. These large remote volunteer contributors may be particularly needed as many aid organizations only have one or two hired GIS practitioners \textcite[p. 2798]{soden_infrastructure_2016}. During the 2015 Nepal earthquake, for example, over 8000 volunteers were mobilized to contribute data such as roads and building footprints for the affected areas \textcite{soden_infrastructure_2016}. Thirdly, the data is often more collaboratively produced \parencite{poiani_potential_2016}. While all of OSM is a collaborative effort between the entire community of contributors, geographically dispersed humanitarian mapmakers must often be more explicitly coordinated through tools such as the HOT Tasking Manager and mailing list \parencite{palen_success_2015}. Lastly, areas under humanitarian focus are likely to show a distinct spatial pattern, whereby an area of interest has much more coverage than nearby regions on the map \parencite{anderson_crowd_2018}. 

These distinct characteristics of data production in OSM during humanitarian mapping efforts have implications for the quality of the data. When we consider data quality as a dataset’s ‘fitness for use’, the rapidly changing circumstances in crisis scenarios means that elements of temporal data quality are particularly relevant \parencite{chen_coordination_2008}. The rapid mapping work done by volunteer communities in the immediate wake of a disaster may provide more up-to-date spatial data than previously existed \parencite{soden_crowdsourced_2014}. For this reason, the Government of Nepal’s Survey Department published maps containing OSM data on official platforms following the 2015 earthquake \parencite{soden_infrastructure_2016}. However, this rapid updating of data by remote volunteers may mean that it is challenging to be maintained over time (especially if it is very detailed), and is thus more likely to be out-of-date when the crisis subsides. Efforts that directly assess the quality of data contributed to OSM in humanitarian mapping efforts have been limited. Moreover, we see a greater need for research that directly addresses issues of data maintenance during these mapping efforts. 

\section{Summary of research opportunity}

OSM is a valuable data source that is particularly relevant in humanitarian and disaster relief operations. However, OSM’s lack of official mechanisms for quality assurance and crowdsourced nature raises many concerns around its quality. As evidenced by the large volume of past work, thinking about geospatial data quality of OSM is not new. However, as the project matures, we must turn our focus towards different elements of data quality than before. Where efforts were once seeking to understand the positional accuracy and completeness of OSM, as in \textcite{haklay_how_2010} and \textcite{girres_quality_2010}, larger volumes of contributions make elements such as temporal accuracy more necessary. 