\chapter{Discussion}
\label{chapterlabel6}

In this chapter we consider the results of our analysis with respect to our research question. We interpret our findings with respect to the existing body of literature that was discussed in Chapter \ref{chapterlabel2}. We also discuss the limitations of this research approach. 

\section{Addressing research questions}

\noindent\textbf{RQ 1: What are the characteristics of data production in our selected humanitarian mapping campaigns and how does this compare with our reference case study?} 

Our results show distinct differences between the humanitarian mapping campaigns and the Heidelberg reference. It is immediately apparent that the daily volume of data created reaches much higher levels during the humanitarian campaigns than in Heidelberg. For example, all humanitarian cases have at least one day where over 2000 features were created, while Heidelberg's maximum is only little over 100 contributions in a single day. Mapping efforts in Kathmandu, Port au Prince, and Tacloban also attracted large volumes of contributors on some days (reaching over 150 unique contributors in a single day in Kathmandu, for example), while efforts in Bangui and Heidelberg had less than 10 unique contributors each day. 

The event/mission classification scheme proposed by \textcite{dittus_mass_2017} offers a useful framework for considering data production in humanitarian mapping campaigns. The early peaks for the event-style campaigns (Port au Prince, Tacloban, and Kathmandu) shown in Figure \ref{fig:time} clearly demonstrate the dramatic burstiness that occurs when mapping is done urgently in response to an immediate natural disaster. We see that the urgency of these events leads to a rapid and significant decay in contribution volume as time passes. As shown in Figure \ref{fig:scatter}, this decay in contribution volume is correlated with a decay in contributor numbers (as is intuitive). This pattern in mapping activity from event style campaigns echoes the findings from \textcite[p. 1294]{dittus_mass_2017}.

While both Heidelberg and Bangui are classified as mission-style campaigns, Figures \ref{fig:time} and \ref{fig:scatter} show notable differences in the dynamics of how data is produced over time. Figure \ref{fig:time} shows that the volume of features created over time in Heidelberg remains relatively stable, while mapping in Bangui has significant peaks throughout the duration of the campaign. It is likely that some of these peaks are the result of data imports. This hypothesis is supported by Figure \ref{fig:scatter}, where we see the relatively weak relationship between the number of daily contributors and the daily volume of contributed data, as some days have over 1000 features produced by fewer than four contributors. As is described in Chapter \ref{chapterlabel5}, data imports from UNICEF have been documented from this campaign. While past works, such as \textcite{ahmouda_analyzing_2018}, aim to remove data imports from their analysis, we consider this to be an important part of the data production landscape that should be considered. Despite these differences, we can see that the mission-style of humanitarian mapping (as represented by the Bangui case study) is more similar to what we would consider as standard practices of OpenStreetMap data production (as represented by the Heidelberg case study) than event-style campaigns.

A review of the commonly-occurring tags keys shows how features such as buildings and roads are common across all case studies. This is not an unexpected finding, as these tags are among the most frequently used in all of OSM. The TagInfo\footnote{\url{https://taginfo.openstreetmap.org/}} website indicates that 58\% of all ways are tagged with \texttt{building} and 24\% of all ways are tagged with \texttt{highway}. Table \ref{tab:tags} also shows the different ways that tags are used to label OSM entities, across both our humanitarian and reference cases. Some tags, such as \texttt{building} and \texttt{highway}, correspond to geographic features that indicate what the OSM entity is representing. Other tags, such as \texttt{source} and \texttt{created_by} indicate characteristics of how the data was produced. Our review of the commonly-used \texttt{source} tags in Table \ref{tab:sources} shows that many of the features from our humanitarian cases were produced from satellite sources, indicating that many contributors were remote. The prevalence of remote contributors during humanitarian mapping campaigns is well-understood within the literature \parencite{dittus_mass_2017, eckle_quality_2015}. While very few of the features from Heidelberg were tagged with a \texttt{source}, we do not see the presence of any satellite sources, indicating that this data was more likely to be produced by local mappers. Interestingly, we also see the presence of disaster-related tags in the humanitarian cases, such as \texttt{}{typhoon:damage, idp:camp_site}, and \texttt{damage:event}. These disaster tags correspond to temporary attributes, suggesting that they will likely need to be removed or updated in the future. \\

\noindent\textbf{RQ 2: To what extent is the data produced during our selected campaigns maintained over time and how does this compare with our reference case study?}

In this work we present multiple approaches for quantifying data maintenance following each of our case study mapping campaigns in OSM. We consider maintenance from both a binary and categorical perspective, acknowledging that a given feature may be updated (ie. maintained) any number of times and thus different degrees of data maintenance can take place. 

Figures \ref{fig:tot}, \ref{fig:dist}, \ref{fig:types}, and \ref{fig:feats} all show how the data from our Heidelberg reference has been maintained to a significantly greater extent than the data from our humanitarian case studies. Heidelberg is the only case study where over 50\% of the features have been updated or deleted at least once since being created. This finding is in line with our expectations, as we have selected Heidelberg as a reference due to its comparatively complete and accurate data \parencite{arsanjani_assessing_2013}. While we are careful not to generalize our findings beyond the case studies that we have selected, this result suggests that OSM data produced from humanitarian mapping efforts may be less maintained than other regions of the database. Greater effort may thus be needed in ensuring that the data produced in response to humanitarian need is maintained in OSM in the years following the campaign. 

These findings may have implications for our understanding of the temporal accuracy of OSM data in areas with humanitarian mapping campaigns. In Chapter \ref{chapterlabel2}, we discussed how data maintenance can be considered as the process by which OSM data is kept up-to-date. Thus, we assume that our case studies found to have poorly maintained data, such as in Tacloban and Bangui, may be more likely to have data that is out of date. This approach to understanding temporal accuracy advances existing work by considering not only the attributes of a given feature within OSM at a given point in time (as in \textcite{barron_comprehensive_2014}, who look at the date of last edit), but also the entire lineage of that feature over its history. This approach acknowledges the ongoing evolution of data within OSM, leading to a deeper understanding of its temporal dimensions. 

However, it is difficult to identify the amount of data maintenance that is necessary in a given area to keep the OSM data up to date. Theoretically, we understand that data only needs to be maintained if the associated geographic phenomena have changed in some way. We assume that it is incredibly unlikely for all geographic phenomena in an area to remain the same over a long period of time, so we understand that, given the passing of time, some data maintenance will always be necessary. However, given that we have no "ground-truth" for how much change has occurred in a given area, it is challenging to know whether or not an apparent lack of data maintenance is a problem. However, our Heidelberg reference case offers an indication of data maintenance levels that might be appropriate. As the OSM data from Heidelberg is generally considered to be of high quality, we might assume that the levels of maintenance seen here are what other regions on the map should strive to achieve. Nevertheless, we also acknowledge that data maintenance needs may vary significantly across different locations. The prevalence of temporally-sensitive tags such as, \texttt{typhoon: damage} and \texttt{damage:event}, in the data from Tacloban may suggest that OSM data produced during humanitarian mapping campaigns may in fact be in need of more maintenance than other parts of the map.

Our findings also suggest a potential shortcomings with current mechanisms of data production during humanitarian mapping campaigns. As is shown by our case studies, these campaigns may produce an incredibly large volume of data over short periods of time (eg. nearly 40,000 nodes and ways produced in Kathmandu alone in less than one year). While there is no denying that this data is immediately useful in the wake of a disaster, it also presents a challenge in that there is now more data that can potentially be out of date in the future if it is not well maintained. Ideally the data produced during a humanitarian campaign is of lasting value to the local community, so care should be taken in how it is maintained. \\

\noindent\textbf{RQ 3: Do our results offer any insight into potential relationships between characteristics of data production and prevalence of data maintenance in each of our case studies?}

It can logically be assumed that low levels of data maintenance are the result of one of two things: 1) there is no need for maintenance as the underlying geography of a given region remains the same, or 2) there is no one with the necessary skills or motivation to do the work involved in maintaining data. 

Our intent is for this work to form a foundation for future efforts that seek to more closely examine data maintenance practices

While stakeholders in the humanitarian mapping community, such as HOT, are well-aware of the importance of engaging local communities of contributors in data production, the justification for doing so is not necessarily framed in terms of data maintenance. \textbf{Rather, it may be more about the anti-colonial values where 
Our work offers empirical justification for the importance of }

-	Heidelberg likely has more maintenance because most of the data is produced by the local community. The original producers of the data are the ones who are also invested in its up-to-dateness. More of the humanitarian data was produced by remote contributors who are not actively invested in its quality years after the initial activation. 
-	Both the Kathmandu and Port au Prince cases have been documented as having significant local engagement (Soden and Palen, 2014), so this may help to explain why this data has been maintained more over time. 
-	Compared against mapping in Tacloban, we see that Kathmandu and Port au Prince took place over longer periods of time and engaged more unique contributors. 
-	Tacloban had the lowest levels of data maintenance and was the case study with the shortest ‘burst’ value. This data was produced incredibly quickly, which may indicate that less effort was made in engaging with local mappers who would take care of it over time. 
-	Within this small sample, we see that larger volumes of data created do not necessarily correspond to less maintenance. 


\section{Project limitations}

We note that, as indicated by the approach taken in \textcite{mooney_characteristics_2012}, a higher number of versions for a given feature may not correspond to our definition of maintenance. Many versions may also correspond to cases where the feature needs to be revised, perhaps due to errors made in the initial mapping efforts or to disagreements in how the feature should best be mapped. In the case of humanitarian mapping, the \textit{validation} process also requires that features be reviewed, which may result in new, corrected versions for a given feature (which is, again, not necessarily maintenance). 

Our work is also limited by the scope of our case studies. Generalizability, etc. 



